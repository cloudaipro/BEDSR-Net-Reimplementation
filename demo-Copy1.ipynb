{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IsHYuhi/BEDSR-Net_A_Deep_Shadow_Removal_Network_from_a_Single_Document_Image/blob/dev%2Ffix/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw6tGGNgMR2K"
   },
   "source": [
    "# BEDSR-Net: A Deep Shadow Removal Network from a Single Document Image\n",
    "\n",
    "## Yun-Hsuan Lin, Wen-Chin Chen, Yung-Yu Chuang, National Taiwan University,\n",
    "\n",
    "### This colab. notebook contains the demo of unofficial reimplementation Lin's CVPR 2020 paper by [IsHYuhi](https://github.com/IsHYuhi). \n",
    "### More detail can be found in [Paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Lin_BEDSR-Net_A_Deep_Shadow_Removal_Network_From_a_Single_Document_CVPR_2020_paper.html). For detail of the code, check the [repo](https://github.com/IsHYuhi/BEDSR-Net_A_Deep_Shadow_Removal_Network_from_a_Single_Document_Image).\n",
    "\n",
    "\\\\\n",
    "\n",
    "### Note\n",
    "The results obtained from Jung dataset.\n",
    "\n",
    "you can test your own images using my pretrained model, check the section  \"Testing your own image\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dIgTpB9zeDg",
    "outputId": "d8a8df6c-e387-4057-d8ac-7d3288ba38ce"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m benet \u001b[38;5;241m=\u001b[39m get_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcam_benet\u001b[39m\u001b[38;5;124m'\u001b[39m, in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m benet\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m benet\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 51\u001b[0m srnet \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrnet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m generator, discriminator \u001b[38;5;241m=\u001b[39m srnet[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)), srnet[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     53\u001b[0m generator\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/PycharmProjects/BEDSR-Net-Reimplementation/libs/models/__init__.py:32\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(name, in_channels, pretrained)\u001b[0m\n\u001b[1;32m     29\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m will be used as a model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrnet\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstcgan\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     discriminator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(models, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscriminator\u001b[39m\u001b[38;5;124m\"\u001b[39m)(pretrained\u001b[38;5;241m=\u001b[39mpretrained)\n\u001b[1;32m     34\u001b[0m     model \u001b[38;5;241m=\u001b[39m [generator, discriminator]\n",
      "File \u001b[0;32m~/PycharmProjects/BEDSR-Net-Reimplementation/libs/models/models.py:264\u001b[0m, in \u001b[0;36mgenerator\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerator\u001b[39m(pretrained: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator:\n\u001b[0;32m--> 264\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m    266\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pretrained/pretrained_g_srnet.prm\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device))\n",
      "File \u001b[0;32m~/PycharmProjects/BEDSR-Net-Reimplementation/libs/models/models.py:176\u001b[0m, in \u001b[0;36mGenerator.__init__\u001b[0;34m(self, in_channels, out_channels)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCvT6 \u001b[38;5;241m=\u001b[39m CvTi(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m\"\u001b[39m, after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCvT7 \u001b[38;5;241m=\u001b[39m CvTi(\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m\"\u001b[39m, after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCvT8 \u001b[38;5;241m=\u001b[39m \u001b[43mCvTi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbefore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReLU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCvT9 \u001b[38;5;241m=\u001b[39m CvTi(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m128\u001b[39m, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m\"\u001b[39m, after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCvT10 \u001b[38;5;241m=\u001b[39m CvTi(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m64\u001b[39m, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m\"\u001b[39m, after\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/BEDSR-Net-Reimplementation/libs/models/models.py:127\u001b[0m, in \u001b[0;36mCvTi.__init__\u001b[0;34m(self, in_channels, out_channels, before, after, kernel_size, stride, padding, dilation, groups, bias)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter: Any[Callable]\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore: Any[Callable]\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvTranspose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mapply(weights_init(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBN\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Volumes/USBDrive/miniconda3/envs/ShadocNet/lib/python3.10/site-packages/torch/nn/modules/conv.py:940\u001b[0m, in \u001b[0;36mConvTranspose2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias, dilation, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m dilation \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[1;32m    939\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m _pair(output_padding)\n\u001b[0;32m--> 940\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/USBDrive/miniconda3/envs/ShadocNet/lib/python3.10/site-packages/torch/nn/modules/conv.py:625\u001b[0m, in \u001b[0;36m_ConvTransposeNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m padding mode is supported for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[1;32m    624\u001b[0m factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/USBDrive/miniconda3/envs/ShadocNet/lib/python3.10/site-packages/torch/nn/modules/conv.py:144\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/USBDrive/miniconda3/envs/ShadocNet/lib/python3.10/site-packages/torch/nn/modules/conv.py:150\u001b[0m, in \u001b[0;36m_ConvNd.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/Volumes/USBDrive/miniconda3/envs/ShadocNet/lib/python3.10/site-packages/torch/nn/init.py:419\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    417\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from libs.models import get_model\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    Resize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from utils.visualize import visualize, reverse_normalize\n",
    "from libs.dataset import get_dataloader\n",
    "from libs.loss_fn import get_criterion\n",
    "from libs.helper_bedsrnet import do_one_iteration\n",
    "\n",
    "def convert_show_image(tensor, idx=None):\n",
    "    if tensor.shape[1]==3:\n",
    "        img = reverse_normalize(tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    elif tensor.shape[1]==1:\n",
    "        img = tensor*0.5+0.5\n",
    "\n",
    "    if idx is not None:\n",
    "        img = (img[idx].transpose(1, 2, 0)*255).astype(np.uint8)\n",
    "    else:\n",
    "        img = (img.squeeze(axis=0).transpose(1, 2, 0)*255).astype(np.uint8)\n",
    "\n",
    "    return img\n",
    "\n",
    "test_transform = Compose([Resize(1024, 768), Normalize(mean=(0.5, ), std=(0.5, )), ToTensorV2()])\n",
    "test_loader = get_dataloader(\n",
    "        \"Jung\",\n",
    "        \"bedsrnet\",\n",
    "        \"test\",\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        transform=test_transform,\n",
    "    )\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "benet = get_model('cam_benet', in_channels=3, pretrained=True)\n",
    "benet.model = benet.model.to(device)\n",
    "srnet = get_model('srnet', pretrained=True)\n",
    "generator, discriminator = srnet[0].to(torch.device('cpu')), srnet[1].to(torch.device('cpu'))\n",
    "generator.eval()\n",
    "discriminator.eval()\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "criterion = get_criterion(\"GAN\", device)\n",
    "lambda_dict = {\"lambda1\": 1.0, \"lambda2\": 0.01}\n",
    "\n",
    "gts = []\n",
    "preds = []\n",
    "attmaps = []\n",
    "bgcolors = []\n",
    "psnrs = []\n",
    "ssims = []\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(test_loader):\n",
    "        print(sample[\"img_path\"][0])\n",
    "        _, _, _, input, gt, pred, attention_map, back_ground, psnr, ssim = do_one_iteration(sample, generator, discriminator, benet, criterion, device, \"evaluate\", lambda_dict)\n",
    "\n",
    "        gts += list(gt)\n",
    "        preds += list(pred)\n",
    "        attmaps += list(attention_map)\n",
    "        bgcolors += list(back_ground)\n",
    "        psnrs.append(psnr)\n",
    "        ssims.append(ssim)\n",
    "\n",
    "print(f\"psnr: {np.mean(psnrs)}\")\n",
    "print(f\"ssim: {np.mean(ssims)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE9lXMo-MR2N"
   },
   "source": [
    "# Results from the testing set of Jung.\n",
    "left to right: input, ground trugh, removal image, background color, attention map from BE-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A4zvh8_MMR2O",
    "outputId": "15d1f287-0d6d-45c1-d08e-86813516bae7"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (9*3, 2*3*len(test_loader)))\n",
    "\n",
    "for idx, sample in enumerate(test_loader):\n",
    "    img_path = sample['img_path'][0].split('/')[-1]\n",
    "\n",
    "    plt.subplot(len(test_loader), 5, idx*5+1)\n",
    "    plt.title(img_path + ' input image')\n",
    "    plt.imshow(convert_show_image(sample[\"img\"].clone().cpu().numpy()))\n",
    "\n",
    "    plt.subplot(len(test_loader), 5, idx*5+2)\n",
    "    plt.title(img_path + ' Ground-Truth image')\n",
    "    plt.imshow(convert_show_image(np.array(gts), idx=idx))\n",
    "\n",
    "    plt.subplot(len(test_loader), 5,  idx*5+3)\n",
    "    plt.title(img_path + ' shadow removal image')\n",
    "    plt.imshow(convert_show_image(np.array(preds), idx=idx))\n",
    "\n",
    "    plt.subplot(len(test_loader), 5, idx*5+4)\n",
    "    plt.title(img_path + ' back ground color image')\n",
    "    plt.imshow(convert_show_image(np.array(bgcolors), idx=idx))\n",
    "\n",
    "    plt.subplot(len(test_loader), 5, idx*5+5)\n",
    "    plt.title(img_path + ' attention map')\n",
    "    plt.imshow(convert_show_image(np.array(attmaps), idx=idx), cmap='jet', alpha=0.5)\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zANevPY2MR2O"
   },
   "source": [
    "# Input Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_o1hp9rZMR2O",
    "outputId": "f43dcfce-b590-4f19-aea4-c57314d0434a"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6*5, 6*5))\n",
    "\n",
    "for idx, sample in enumerate(test_loader):\n",
    "    plt.subplot(4, 5, idx+1)\n",
    "    plt.title(sample['img_path'][0].split('/')[-1])\n",
    "    plt.imshow(convert_show_image(sample[\"img\"].clone().detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejE8q7BnMR2P"
   },
   "source": [
    "# Output Shadow Removal Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xDRF4Xa9MR2P",
    "outputId": "176abd50-bd91-4407-b174-cbeec947afe3"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6*5, 6*5))\n",
    "\n",
    "for idx, sample in enumerate(test_loader):\n",
    "    plt.subplot(4, 5, idx+1)\n",
    "    plt.title(sample['img_path'][0].split('/')[-1])\n",
    "    plt.imshow(convert_show_image(np.array(preds), idx=idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6rhmTNXMR2P"
   },
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HZXpIopNMR2P",
    "outputId": "a6d84996-948c-4bae-bbde-e3ac268fa4d1"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6*5, 6*5))\n",
    "\n",
    "for idx, sample in enumerate(test_loader):\n",
    "    plt.subplot(4, 5, idx+1)\n",
    "    plt.title(sample['img_path'][0].split('/')[-1])\n",
    "    plt.imshow(convert_show_image(np.array(gts), idx=idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yft4zXhqMR2Q"
   },
   "source": [
    "# Attention Map from BE-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LQEu6ZV2MR2Q",
    "outputId": "4a8de5c3-d7f5-48e0-c3aa-2d0e862d2348"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6*5, 6*5))\n",
    "\n",
    "for idx, sample in enumerate(test_loader):\n",
    "    plt.subplot(4, 5, idx+1)\n",
    "    plt.title(sample['img_path'][0].split('/')[-1])\n",
    "    plt.imshow(convert_show_image(np.array(attmaps), idx=idx), cmap='jet', alpha=0.5)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHBTruexMR2Q"
   },
   "source": [
    "# Back Ground Color from BE-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ikDo8WGTMR2Q",
    "outputId": "e8d46a64-2295-4180-b828-45e9b6e7e57a"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6*5, 6*5))\n",
    "\n",
    "for idx, sample in enumerate(test_loader):\n",
    "    plt.subplot(4, 5, idx+1)\n",
    "    plt.title(sample['img_path'][0].split('/')[-1])\n",
    "    plt.imshow(convert_show_image(np.array(bgcolors), idx=idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDhubTwWMR2R"
   },
   "source": [
    "# Testing your own image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvsVrxU11Wb6",
    "outputId": "b04792a2-6725-4941-ae2d-ca9746d7c541"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwpbhbaAMR2R"
   },
   "outputs": [],
   "source": [
    "# you can put your own image path.\n",
    "result_path = '../drive/MyDrive/shadow_removal_image.jpg'\n",
    "image = cv2.imread(\"../drive/MyDrive/shadow_image.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "h, w, c = image.shape\n",
    "\n",
    "tensor = test_transform(image=image)\n",
    "tensor = tensor['image'].unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBtazF1nMR2R",
    "outputId": "d4194a8a-fc45-4462-e288-be9e22e8870e"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        color, attmap, _ = benet(tensor)\n",
    "        attmap = (attmap-0.5)/0.5\n",
    "        back_color = torch.repeat_interleave(color.detach(), 1024*768, dim=0)\n",
    "        back_ground = back_color.reshape(1, c, 1024, 768).to(device)\n",
    "\n",
    "    input = torch.cat([tensor, attmap, back_ground], dim=1)\n",
    "\n",
    "    tensor = tensor.detach().cpu()\n",
    "    attmap = attmap.detach().cpu()\n",
    "    back_ground = back_ground.detach().cpu()\n",
    "    shadow_removal_image = generator(input).detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "-OCY2LaqMR2R",
    "outputId": "cc84d325-fa53-47cd-f7f5-075dc368bfc7"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (9*3, 2*3))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('input image')\n",
    "plt.imshow(convert_show_image(tensor.clone().detach().cpu().numpy()))\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('shadow removal image')\n",
    "removal = convert_show_image(shadow_removal_image.clone().detach().cpu().numpy())\n",
    "plt.imshow(removal)\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('back ground color image')\n",
    "plt.imshow(convert_show_image(back_ground.clone().detach().cpu().numpy()))\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('attention map')\n",
    "plt.imshow(convert_show_image(attmap.clone().detach().cpu().numpy()), cmap='jet', alpha=0.5)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jalwHrE9MR2S"
   },
   "source": [
    "### saving result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVN5f-_wMR2S",
    "outputId": "c087922d-7eac-4230-c489-5f78f1e72dd6"
   },
   "outputs": [],
   "source": [
    "if cv2.imwrite(result_path, cv2.cvtColor(removal, cv2.COLOR_RGB2BGR)):\n",
    "  print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwlVLAgz2CrP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
